{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1: Trump, Twitter, and Text\n",
    "\n",
    "Welcome to the first project of DS100! In this project, we will work with the Twitter API in order to analyze Donald Trump's tweets.\n",
    "\n",
    "**The project checkpoint is due 11:59pm Tuesday, Sept. 26.** The checkpoint is graded on *completeness*, not correctness. You must complete Questions 1-4 in order to get full credit for the checkpoint.\n",
    "\n",
    "**The entire project is due 11:59pm Tuesday, Oct 3.**\n",
    "\n",
    "*Fair warning:* This project involves significantly more challenging pandas operations than the previous homeworks. We strongly suggest you start early.\n",
    "\n",
    "**Grading:**\n",
    "\n",
    "The checkpoint is worth **10%** of the overall project grade and is graded on completeness.\n",
    "\n",
    "However, the final submission of the project will be graded on **correctness**.\n",
    "\n",
    "**Fun:**\n",
    "\n",
    "We intended this project to be fun! You will analyze actual data from the Twitter API â€” no precooked dataset this time around. You will also draw conclusions about the current (and often controversial) US President's tweet behavior. If you find yourself getting frustrated or stuck on one problem for too long, we suggest coming into office hours and working with friends in the class.\n",
    "\n",
    "With that in mind, let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Convenience function for instructor use only.\n",
    "# Call this function to force refresh the okpy tests.\n",
    "def refresh():\n",
    "    import sys\n",
    "    keys = [k for k in sys.modules.keys() if 'ok_tests' in k]\n",
    "    for k in keys:\n",
    "        del sys.modules[k]\n",
    "    global ok\n",
    "    ok = Notebook('proj1.ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to set up your notebook\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('fivethirtyeight')\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "sns.set_context(\"talk\")\n",
    "import re\n",
    "\n",
    "from IPython.display import display, Latex, Markdown\n",
    "from client.api.notebook import Notebook\n",
    "ok = Notebook('proj1.ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log into OkPy. You may have to change this to ok.auth(force=True) if you run into\n",
    "# an OAuthError\n",
    "ok.auth()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br/><br/><br/>\n",
    "\n",
    "# Getting the dataset\n",
    "\n",
    "---\n",
    "\n",
    "Since we'll be looking at Twitter data, we need to download the data from Twitter!\n",
    "\n",
    "Twitter provides an API for downloading tweet data in large batches.  The `tweepy` package makes it fairly easy to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup\n",
    "Install `tweepy`, if you don't already have it.  (Be sure to activate your Conda environment for the class first.  Then run `pip install tweepy`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Make sure you are in your ds100 conda environment.\n",
    "## Uncomment the following line to install tweepy\n",
    "# !pip install tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The following should run\n",
    "import tweepy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are instructions on using `tweepy` [here](http://tweepy.readthedocs.io/en/v3.5.0/getting_started.html), but we will give you example code.\n",
    "\n",
    "Twitter requires you to have authentication keys to access their API.  To get your keys, you'll have to sign up as a Twitter developer.  The next question will walk you through this process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br/><br/>\n",
    "\n",
    "## Question 1\n",
    "\n",
    "Follow the instructions below to get your Twitter API keys.  Read the instructions completely before starting.\n",
    "\n",
    "1. [Create a Twitter account](https://twitter.com).  You can use an existing account if you have one.\n",
    "2. Under account settings, add your phone number to the account.\n",
    "3. [Create a Twitter developer account](https://dev.twitter.com/resources/signup).  Attach it to your Twitter account.\n",
    "4. Once you're logged into your developer account, [create an application for this assignment](https://apps.twitter.com/app/new).  You can call it whatever you want, and you can write any URL when it asks for a web site.\n",
    "5. On the page for that application, find your Consumer Key and Consumer Secret.\n",
    "6. On the same page, create an Access Token.  Record the resulting Access Token and Access Token Secret.\n",
    "7. Edit the file [keys.json](keys.json) and replace the placeholders with your keys.  Don't turn in that file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## WARNING (Please Read) !!!!\n",
    "\n",
    "\n",
    "### Protect your Twitter Keys\n",
    "<span style=\"color:red\">\n",
    "If someone has your authentication keys, they can access your Twitter account and post as you!  So don't give them to anyone, and **don't write them down in this notebook**. \n",
    "</span>\n",
    "The usual way to store sensitive information like this is to put it in a separate file and read it programmatically.  That way, you can share the rest of your code without sharing your keys.  That's why we're asking you to put your keys in `keys.json` for this assignment.\n",
    "\n",
    "\n",
    "### Avoid making too many API calls.\n",
    "\n",
    "<span style=\"color:red\">\n",
    "Twitter limits developers to a certain rate of requests for data.  If you make too many requests in a short period of time, you'll have to wait awhile (around 15 minutes) before you can make more.  </span> \n",
    "So carefully follow the code examples you see and don't rerun cells without thinking.  Instead, always save the data you've collected to a file.  We've provided templates to help you do that.\n",
    "\n",
    "\n",
    "### Be careful about which functions you call!\n",
    "\n",
    "<span style=\"color:red\">\n",
    "This API can retweet tweets, follow and unfollow people, and modify your twitter settings.  Be careful which functions you invoke! </span> One of your instructors accidentally re-tweeted some tweets because that instructor typed `retweet` instead of `retweet_count`. \n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "key_file = 'keys.json'\n",
    "# Loading your keys from keys.json (which you should have filled\n",
    "# in in question 1):\n",
    "with open(key_file) as f:\n",
    "    keys = json.load(f)\n",
    "# if you print or view the contents of keys be sure to delete the cell!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell tests the Twitter authentication. It should run without errors or warnings and display your Twitter username."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from tweepy import TweepError\n",
    "import logging\n",
    "\n",
    "try:\n",
    "    auth = tweepy.OAuthHandler(keys[\"consumer_key\"], keys[\"consumer_secret\"])\n",
    "    auth.set_access_token(keys[\"access_token\"], keys[\"access_token_secret\"])\n",
    "    api = tweepy.API(auth)\n",
    "    print(\"Your username is:\", api.auth.get_username())\n",
    "except TweepError as e:\n",
    "    logging.warning(\"There was a Tweepy error. Double check your API keys and try again.\")\n",
    "    logging.warning(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br/><br/>\n",
    "\n",
    "\n",
    "## Question 2\n",
    "\n",
    "In the example below, we have loaded some tweets by @BerkeleyData.  Run it and read the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "ds_tweets_save_path = \"BerkeleyData_recent_tweets.pkl\"\n",
    "# Guarding against attempts to download the data multiple\n",
    "# times:\n",
    "if not Path(ds_tweets_save_path).is_file():\n",
    "    # Obtaining twitter keys\n",
    "    import json\n",
    "    key_file = 'keys.json'\n",
    "    # Loading your keys from keys.json (which you should have filled\n",
    "    # in in question 1):\n",
    "    with open(key_file) as f:\n",
    "        keys = json.load(f)\n",
    "    \n",
    "    # Authenticating:\n",
    "    import tweepy\n",
    "    auth = tweepy.OAuthHandler(keys[\"consumer_key\"], keys[\"consumer_secret\"])\n",
    "    auth.set_access_token(keys[\"access_token\"], keys[\"access_token_secret\"])\n",
    "    api = tweepy.API(auth)\n",
    "\n",
    "    # Getting as many recent tweets by @BerkeleyData as Twitter will let us have:\n",
    "    example_tweets = list(tweepy.Cursor(api.user_timeline, id=\"BerkeleyData\").items())\n",
    "\n",
    "    # Saving the tweets to a file as \"pickled\" objects:\n",
    "    with open(ds_tweets_save_path, \"wb\") as f:\n",
    "        import pickle\n",
    "        pickle.dump(example_tweets, f)\n",
    "\n",
    "# Re-loading the results:\n",
    "with open(ds_tweets_save_path, \"rb\") as f:\n",
    "    import pickle\n",
    "    example_tweets = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming everything ran correctly you should be able to look at the first tweet by running the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Looking at one tweet object, which has type Status: \n",
    "from pprint import pprint # ...to get a more easily-readable view.\n",
    "pprint(example_tweets[0]._json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### What you need to do. \n",
    "\n",
    "Re-factor the above code fragment into reusable snippets below.  You should not need to make major modifications; this is mostly an exercise in understanding the above code block. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "def load_keys(path):\n",
    "    \"\"\"Loads your Twitter authentication keys from a file on disk.\n",
    "    \n",
    "    Args:\n",
    "        path (str): The path to your key file.  The file should\n",
    "          be in JSON format and look like this (but filled in):\n",
    "            {\n",
    "                \"consumer_key\": \"<your Consumer Key here>\",\n",
    "                \"consumer_secret\":  \"<your Consumer Secret here>\",\n",
    "                \"access_token\": \"<your Access Token here>\",\n",
    "                \"access_token_secret\": \"<your Access Token Secret here>\"\n",
    "            }\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary mapping key names (like \"consumer_key\") to\n",
    "          key values.\"\"\"\n",
    "    \n",
    "    import json\n",
    "    with open(path) as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "def download_recent_tweets_by_user(user_account_name, keys):\n",
    "    \"\"\"Downloads tweets by one Twitter user.\n",
    "\n",
    "    Args:\n",
    "        user_account_name (str): The name of the Twitter account\n",
    "          whose tweets will be downloaded.\n",
    "        keys (dict): A Python dictionary with Twitter authentication\n",
    "          keys (strings), like this (but filled in):\n",
    "            {\n",
    "                \"consumer_key\": \"<your Consumer Key here>\",\n",
    "                \"consumer_secret\":  \"<your Consumer Secret here>\",\n",
    "                \"access_token\": \"<your Access Token here>\",\n",
    "                \"access_token_secret\": \"<your Access Token Secret here>\"\n",
    "            }\n",
    "\n",
    "    Returns:\n",
    "        list: A list of Status objects, each representing one tweet.\"\"\"\n",
    "    import tweepy\n",
    "\n",
    "    # Authenticating:\n",
    "    auth = tweepy.OAuthHandler(keys[\"consumer_key\"], keys[\"consumer_secret\"])\n",
    "    auth.set_access_token(keys[\"access_token\"], keys[\"access_token_secret\"])\n",
    "    api = tweepy.API(auth)\n",
    "\n",
    "    return list(tweepy.Cursor(api.user_timeline, id=user_account_name).items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "def save_tweets(tweets, path):\n",
    "    \"\"\"Saves a list of tweets to a file in the local filesystem.\n",
    "    \n",
    "    This function makes no guarantee about the format of the saved\n",
    "    tweets, **except** that calling load_tweets(path) after\n",
    "    save_tweets(tweets, path) will produce the same list of tweets\n",
    "    and that only the file at the given path is used to store the\n",
    "    tweets.  (That means you can implement this function however\n",
    "    you want, as long as saving and loading works!)\n",
    "\n",
    "    Args:\n",
    "        tweets (list): A list of tweet objects (of type Status) to\n",
    "          be saved.\n",
    "        path (str): The place where the tweets will be saved.\n",
    "\n",
    "    Returns:\n",
    "        None\"\"\"\n",
    "    with open(path, \"wb\") as f:\n",
    "        import pickle\n",
    "        pickle.dump(tweets, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "def load_tweets(path):\n",
    "    \"\"\"Loads tweets that have previously been saved.\n",
    "    \n",
    "    Calling load_tweets(path) after save_tweets(tweets, path)\n",
    "    will produce the same list of tweets.\n",
    "    \n",
    "    Args:\n",
    "        path (str): The place where the tweets were be saved.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of Status objects, each representing one tweet.\"\"\"\n",
    "    with open(path, \"rb\") as f:\n",
    "        import pickle\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "def get_tweets_with_cache(user_account_name, keys_path):\n",
    "    \"\"\"Get recent tweets from one user, loading from a disk cache if available.\n",
    "    \n",
    "    The first time you call this function, it will download tweets by\n",
    "    a user.  Subsequent calls will not re-download the tweets; instead\n",
    "    they'll load the tweets from a save file in your local filesystem.\n",
    "    All this is done using the functions you defined in the previous cell.\n",
    "    This has benefits and drawbacks that often appear when you cache data:\n",
    "    \n",
    "    +: Using this function will prevent extraneous usage of the Twitter API.\n",
    "    +: You will get your data much faster after the first time it's called.\n",
    "    -: If you really want to re-download the tweets (say, to get newer ones,\n",
    "       or because you screwed up something in the previous cell and your\n",
    "       tweets aren't what you wanted), you'll have to find the save file\n",
    "       (which will look like <something>_recent_tweets.pkl) and delete it.\n",
    "    \n",
    "    Args:\n",
    "        user_account_name (str): The Twitter handle of a user, without the @.\n",
    "        keys_path (str): The path to a JSON keys file in your filesystem.\n",
    "    \"\"\"\n",
    "    save_path = \"{}_recent_tweets.pkl\".format(user_account_name)\n",
    "    from pathlib import Path\n",
    "    if not Path(save_path).is_file():\n",
    "        keys = load_keys(keys_path)\n",
    "        tweets = download_recent_tweets_by_user(user_account_name, keys)\n",
    "        save_tweets(tweets, save_path)\n",
    "    return load_tweets(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything was implemented correctly you should be able to obtain roughly the last 3000 tweets by the `realdonaldtrump`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When you are done, run this cell to load @realdonaldtrump's tweets.\n",
    "# Note the function get_tweets_with_cache.  You may find it useful\n",
    "# later.\n",
    "trump_tweets = get_tweets_with_cache(\"realdonaldtrump\", key_file)\n",
    "print(\"Number of tweets downloaded:\", len(trump_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false,
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Note that we remove special characters from the tweets to make them printable\n",
    "# in the autograder zip; this was needed in order to make the autograder work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br/><br/>\n",
    "\n",
    "## Question 3\n",
    "\n",
    "In this question you will extract important fields from the tweet objects and convert them into a Pandas dataframe for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each trump tweet is stored in a `tweepy.models.Status` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(trump_tweets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can list all the members of this object by looking at the private `__dict__` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list(trump_tweets[0].__dict__.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore we can extract a field simply by reading its value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump_tweets[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What you need to do\n",
    "\n",
    "Construct a DataFrame called `trump`. The index of the dataframe should be the ID of each tweet (looks something like `907698529606541312`). It should have these columns:\n",
    "\n",
    "- `time`: The time the tweet was created.\n",
    "- `source`: The source device of the tweet.\n",
    "- `text`: The text of the tweet.\n",
    "- `retweet_count`: The retweet count of the tweet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "trump = pd.DataFrame({\n",
    "    'time': [tweet.created_at for tweet in trump_tweets],\n",
    "    'source': [tweet.source for tweet in trump_tweets],\n",
    "    'text': [tweet.text for tweet in trump_tweets],\n",
    "    'retweet_count': [tweet.retweet_count for tweet in trump_tweets],\n",
    "}, index=[tweet.id_str for tweet in trump_tweets],\n",
    "   columns=['time', 'source', 'text', 'retweet_count'],\n",
    ")\n",
    "trump.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "tags": [
     "test",
     "q03"
    ]
   },
   "outputs": [],
   "source": [
    "assert isinstance(trump, pd.DataFrame)\n",
    "assert trump.shape[0] < 3300\n",
    "assert trump.shape[1] >= 4\n",
    "assert '907698529606541312' in trump.index\n",
    "assert '907675638055743489' in trump.index\n",
    "assert all(col in trump.columns for col in ['time', 'source', 'text', 'retweet_count'])\n",
    "# If you fail these tests, you probably tried to use __dict__ or _json to read in the tweets\n",
    "assert 'Twitter for iPhone' in trump['source'].values\n",
    "assert trump['time'].dtype == np.dtype('<M8[ns]')\n",
    "assert trump['text'].dtype == np.dtype('O')\n",
    "assert trump['retweet_count'].dtype == np.dtype('int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are two important dates that we'll use in our analysis. `ELEC_DATE` is the date when Trump won the 2016 Presidential election, and `INAUG_DATE` is the date that Trump was sworn into office."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "ELEC_DATE = datetime(2016, 11, 8)\n",
    "INAUG_DATE = datetime(2017, 1, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the first and last rows of your tweet data.\n",
    "\n",
    "You'll notice that the data contains tweets from before the election."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump.iloc[[0, -1], :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: Tweet Source Analysis\n",
    "\n",
    "In the following questions, we are going to find out the charateristics of Trump tweets and the devices used for the tweets.\n",
    "\n",
    "---\n",
    "<br/><br/>\n",
    "\n",
    "## Question 4a\n",
    "\n",
    "Find out the number of unique sources of the Trump tweets. Save the result in `num_sources`. \n",
    "\n",
    "Then, make a bar plot of the counts of different sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "num_sources = len(trump['source'].unique())\n",
    "\n",
    "sns.countplot(y='source', data=trump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "tags": [
     "test",
     "q04a"
    ]
   },
   "outputs": [],
   "source": [
    "assert num_sources == 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the plot above, Trump tweets are mostly from iPhone or Android. Is there a difference between his Tweet behavior between the two devices?\n",
    "\n",
    "We will attempt to answer this question in our subsequent analysis.\n",
    "\n",
    "First, we'll take a look at whether Trump's tweets from an Android come at different times than his tweets from an iPhone. Note that Twitter gives us his tweets in the [UTC timezone](https://www.wikiwand.com/en/List_of_UTC_time_offsets):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trump_tweets[0]._json['created_at'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll convert the tweet times to US Eastern Time, the timezone of New York and Washington D.C., since those are the places we would expect the most tweet activity from Trump."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump['est_time'] = pd.Index(trump['time']).tz_localize(\"UTC\").tz_convert(\"US/Eastern\")\n",
    "trump.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br/><br/>\n",
    "\n",
    "## Question 4b\n",
    "\n",
    "Use this data to make a line plot with two curves:\n",
    "\n",
    "1. The number of iPhone tweets vs. hour of the day, normalized over the hours of the day. For example, if there were 10 tweets at 1pm and 20 tweets at 2pm, the line plot should be 0, then 0.33 at 1pm, 0.66 at 2pm, then back to 0.\n",
    "2. The same curve for Android tweets.\n",
    "\n",
    "Put the two curves in the same plot. You should get a plot that looks like:\n",
    "\n",
    "![](device_hour.png)\n",
    "\n",
    "Hint: use `value_counts` with `normalization`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution",
     "written"
    ]
   },
   "outputs": [],
   "source": [
    "(trump['est_time'].dt.hour\n",
    " [trump.source.str.contains('iPhone')]\n",
    " .value_counts(normalize=True, sort=False)\n",
    " .sort_index()\n",
    " .plot()\n",
    ")\n",
    "\n",
    "(trump['est_time'].dt.hour\n",
    " [trump.source.str.contains('Android')]\n",
    " .value_counts(normalize=True, sort=False)\n",
    " .sort_index()\n",
    " .plot()\n",
    ")\n",
    "\n",
    "plt.xlabel('hour')\n",
    "plt.ylabel('fraction')\n",
    "plt.legend(['iPhone', 'Android'])\n",
    "\n",
    "plt.savefig('device_hour.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "<br></br>\n",
    "\n",
    "## Question 4c\n",
    "What can you find out in the plot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution",
     "written"
    ]
   },
   "outputs": [],
   "source": [
    "q4c_answer = r\"\"\"\n",
    "    \n",
    "Tweets from Android mostly concentrated in the morning with a clear peak;\n",
    "Tweets from iPhone are distributed more evenly in the working hours.\n",
    "\n",
    "*Alternative answer:*\n",
    "\n",
    "Trump is said to use a Samsung Galaxy Smart Phone.\n",
    "From the above we might conclude that Trump tends to write his own tweets \n",
    "in the morning while his PR team writes tweets during the day \n",
    "(perhaps with a late dinner break)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "display(Markdown(q4c_answer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "<br></br>\n",
    "\n",
    "Let's now look at his tweet device usage over the entire time period we have in the dataset.\n",
    "\n",
    "Take a look at the code below and the plot it generates.\n",
    "\n",
    "You should be answer the following questions about this code. You don't have to write the answers down anywhere, but you'll need to make variations of this plot in later questions so understanding this code will help you greatly.\n",
    "\n",
    "1. What does `set_index` do here?\n",
    "1. What does `resample` do? What does the `'D'` argument do in `resample`?\n",
    "1. What does `unstack` do? What does the `level=0` argument do in unstack?\n",
    "1. Why does one call to `plot()` generate 7 lines?\n",
    "\n",
    "Feel free to copy this cell, play around with the code to see the intermediate result, then delete your cell after you're done.\n",
    "\n",
    "(**Also, all the questions in this project can be answered using this style of method chaining.** If you write `for` loops in this project, know that there is a better way.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trump.loc[:, ['est_time', 'source']]\n",
    " .set_index('est_time')\n",
    " .groupby('source')\n",
    " .resample('D')\n",
    " .size()\n",
    " .unstack(level=0)\n",
    " .plot()\n",
    ")\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4d\n",
    "\n",
    "One problem with the plot above is that it plots too many points to see overall trends in the device usage.\n",
    "\n",
    "Recreate the plot above, grouping by each month instead of each day. You may want to use the `figsize` argument to `plot` to make your plot larger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution",
     "written"
    ]
   },
   "outputs": [],
   "source": [
    "(trump.loc[:, ['est_time', 'source']]\n",
    " .set_index('est_time')\n",
    " .groupby('source')\n",
    " .resample('MS')\n",
    " .size()\n",
    " .rename('count')\n",
    " .unstack(level=0)\n",
    " .plot(figsize=(10, 8))\n",
    ")\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the plot, Trump's tweets come from many different sources. It turns out that many of his tweets were not from Trump himself but from his staff. [Take a look at this Verge article.](https://www.theverge.com/2017/3/29/15103504/donald-trump-iphone-using-switched-android)\n",
    "\n",
    "Does the data support the information in the article? What else do you find out about changes in Trump's tweets sources from the plot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution",
     "written"
    ]
   },
   "outputs": [],
   "source": [
    "q4d_answer = r\"\"\"\n",
    "\n",
    "According to the plot, it looks like Trump stopped using his Android phone\n",
    "and switched to iPhone sometime during March 2017. This supports the information\n",
    "given in the article.\n",
    "\n",
    "We also notice that there used to be many sources of tweets, but nowadays\n",
    "almost all of the tweets come from an iPhone.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "display(Markdown(q4d_answer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are some ways we can distinguish between tweets that came from Trump and tweets from his staff? Before, it was as easy as checked which device the tweet came from. Now, we have to rely on more sophisticated methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Checkpoint Reached\n",
    "\n",
    "Congrats, you just reached the Project 1 checkpoint!\n",
    "\n",
    "Now, run the cell below to submit your assignment to OkPy. The autograder should email you shortly with your autograded score. The autograder will only run once every 30 minutes.\n",
    "\n",
    "Note that you'll probably get 0 points on Questions 5-8. Since you haven't gotten there yet, don't worry about missing those points.\n",
    "\n",
    "**Note that the checkpoint is graded on completeness. This means that we'll check whether you attempted to solve each question, not whether you've gotten them right. When you submit the final submission of the project, you can go back and revise your answers to Questions 1-4.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = ok.submit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br></br>\n",
    "\n",
    "## Question 5: Sentiment Analysis\n",
    "\n",
    "It turns out that we can use the words in Trump's tweets to calculate a measure of the sentiment of the tweet. For example, the sentence \"I love America!\" has positive sentiment, whereas the sentence \"I hate taxes!\" has a negative sentiment. In addition, some words have stronger positive / negative sentiment than others: \"I love America.\" is more positive than \"I like America.\"\n",
    "\n",
    "We will use the [VADER (Valence Aware Dictionary and sEntiment Reasoner)](https://github.com/cjhutto/vaderSentiment) lexicon to analyze the sentiment of Trump's tweets. VADER is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media which is great for our usage.\n",
    "\n",
    "The VADER lexicon gives the sentiment of individual words. Run the following cell to show the first few rows of the lexicon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head vader_lexicon.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br/><br/>\n",
    "\n",
    "## Question 5a\n",
    "\n",
    "As you can see, the lexicon contains emojis too! The first column of the lexicon is the *token*, or the word itself. The second column is the *polarity* of the word, or how positive / negative it is.\n",
    "\n",
    "(How did they decide the polarities of these words? What are the other two columns in the lexicon? See the link above.)\n",
    "\n",
    " Read in the lexicon into a DataFrame called `sent`. The index of the DF should be the tokens in the lexicon. `sent` should have one column: `polarity`: The polarity of each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "sent = pd.read_csv('vader_lexicon.txt', sep='\\t',\n",
    "                   usecols=[0, 1], header=None, names=['token', 'polarity'],\n",
    "                   index_col='token')\n",
    "sent.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "tags": [
     "test",
     "q05a"
    ]
   },
   "outputs": [],
   "source": [
    "assert isinstance(sent, pd.DataFrame)\n",
    "assert sent.shape == (7517, 1)\n",
    "assert list(sent.index[5000:5005]) == ['paranoids', 'pardon', 'pardoned', 'pardoning', 'pardons']\n",
    "assert np.allclose(sent['polarity'].head(), [-1.5, -0.4, -1.5, -0.4, -0.7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br/><br/>\n",
    "\n",
    "## Question 5b\n",
    "\n",
    "Now, let's use this lexicon to calculate the overall sentiment for each of Trump's tweets. Here's the basic idea:\n",
    "\n",
    "1. For each tweet, find the sentiment of each word.\n",
    "2. Calculate the sentiment of each tweet by taking the sum of the sentiments of its words.\n",
    "\n",
    "First, let's lowercase the text in the tweets since the lexicon is also lowercase. Set the `text` column of the `trump` DF to be the lowercased text of each tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "trump['text'] = trump['text'].str.lower()\n",
    "trump.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "tags": [
     "test",
     "q05b"
    ]
   },
   "outputs": [],
   "source": [
    "assert trump['text'].loc['884740553040175104'] == 'working hard to get the olympics for the united states (l.a.). stay tuned!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br/><br/>\n",
    "\n",
    "## Question 5c\n",
    "\n",
    "Now, let's get rid of punctuation since it'll cause us to fail to match words. Create a new column called `no_punc` in the `trump` DF to be the lowercased text of each tweet with all punctuation replaced by a single space. We consider punctuation characters to be any character that isn't a Unicode word character or a whitespace character. You may want to consult the Python documentation on regexes for this problem.\n",
    "\n",
    "(Why don't we simply remove punctuation instead of replacing with a space? See if you can figure this out by looking at the tweet data.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "punct_re = r'[^\\w\\s]'\n",
    "trump['no_punc'] = trump['text'].str.replace(punct_re, ' ') # SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "tags": [
     "test",
     "q05c"
    ]
   },
   "outputs": [],
   "source": [
    "assert isinstance(punct_re, str)\n",
    "assert re.search(punct_re, 'this') is None\n",
    "assert re.search(punct_re, 'this is ok') is None\n",
    "assert re.search(punct_re, 'this is\\nok') is None\n",
    "assert re.search(punct_re, 'this is not ok.') is not None\n",
    "assert re.search(punct_re, 'this#is#ok') is not None\n",
    "assert re.search(punct_re, 'this^is ok') is not None\n",
    "assert trump['no_punc'].loc['907588803161939968'] == 'fascinating to watch people writing books and major articles about me and yet they know nothing about me  amp  have zero access   fake news '\n",
    "assert trump['no_punc'].loc['907675638055743489'] == 'congratulations to eric  amp  lara on the birth of their son  eric  luke  trump this morning  https   t co aw0av82xde'\n",
    "# If you fail these tests, you accidentally changed the text column\n",
    "assert trump['text'].loc['884740553040175104'] == 'working hard to get the olympics for the united states (l.a.). stay tuned!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br/> <br/>\n",
    "\n",
    "## Question 5d:\n",
    "\n",
    "\n",
    "Now, let's convert the tweets into what's called a *tidy format* to make the sentiments easier to calculate. Use the `no_punc` column of `trump` to create a table called `tidy_format`. The index of the table should be the IDs of the tweets, repeated once for every word in the tweet. It has two columns:\n",
    "\n",
    "1. `num`: The location of the word in the tweet. For example, if the tweet was \"i love america\", then the location of the word \"i\" is 0, \"love\" is 1, and \"america\" is 2.\n",
    "2. `word`: The individual words of each tweet.\n",
    "\n",
    "The first few rows of our `tidy_format` table look like:\n",
    "\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>num</th>\n",
    "      <th>word</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>907698529606541312</th>\n",
    "      <td>0</td>\n",
    "      <td>it</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>907698529606541312</th>\n",
    "      <td>1</td>\n",
    "      <td>was</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>907698529606541312</th>\n",
    "      <td>2</td>\n",
    "      <td>a</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>907698529606541312</th>\n",
    "      <td>3</td>\n",
    "      <td>great</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>907698529606541312</th>\n",
    "      <td>4</td>\n",
    "      <td>honor</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "**Note that you'll get different results depending on when you pulled in the tweets.** However, you can double check that your tweet with ID `907698529606541312` has the same rows as ours. Our tests don't check whether your table looks exactly like ours.\n",
    "\n",
    "This will require some rather advanced Pandas hacking, but our solution uses a chain of 5 methods on the `trump` DF.\n",
    "\n",
    "* **Hint 1:** Try looking at the `expand` argument to pandas' `str.split`.\n",
    "\n",
    "* **Hint 2:** Try looking at the `stack()` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "tidy_format = (\n",
    "    trump['no_punc']\n",
    "    .str.split(expand=True)\n",
    "    .stack()\n",
    "    .reset_index(level=1)\n",
    "    .rename(columns={'level_1': 'num', 0: 'word'})\n",
    ")\n",
    "tidy_format.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "tags": [
     "test",
     "q05d"
    ]
   },
   "outputs": [],
   "source": [
    "assert tidy_format.loc['907698529606541312'].shape == (24, 2)\n",
    "assert ' '.join(list(tidy_format.loc['907675638055743489']['word'])) == 'congratulations to eric amp lara on the birth of their son eric luke trump this morning https t co aw0av82xde'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br/> <br/>\n",
    "\n",
    "\n",
    "## Question 5e:\n",
    "\n",
    "Now that we have this table in the tidy format, it becomes much easier to find the sentiment of each tweet: we can join the table with the lexicon table. Calculate a table called `polarities`. Its index should be the IDs of the tweets (one row per ID). It should have one column called `polarity` containing the summed sentiment polarity of each tweet.\n",
    "\n",
    "Here are the first couple rows of our table. Again, you may have different tweet IDs depending on when you downloaded the tweets, but make sure the format looks the same.\n",
    "\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>polarity</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>index</th>\n",
    "      <th></th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>755486029742641152</th>\n",
    "      <td>0.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>755502053003190273</th>\n",
    "      <td>0.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>755525857456844800</th>\n",
    "      <td>2.2</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>755551039244341253</th>\n",
    "      <td>6.7</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>755708398537154560</th>\n",
    "      <td>7.6</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "polarities = (\n",
    "    tidy_format\n",
    "    .merge(sent, how='left', left_on='word', right_index=True)\n",
    "    .reset_index()\n",
    "    .loc[:, ['index', 'polarity']]\n",
    "    .groupby('index')\n",
    "    .sum()\n",
    "    .fillna(0)\n",
    ")\n",
    "polarities.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "tags": [
     "test",
     "q05e"
    ]
   },
   "outputs": [],
   "source": [
    "assert np.allclose(polarities.loc['907698529606541312', 'polarity'], 7.3)\n",
    "assert np.allclose(polarities.loc['907675638055743489', 'polarity'], 2.9)\n",
    "assert np.allclose(polarities.loc['907592460070768641', 'polarity'], 3.2)\n",
    "assert np.allclose(polarities.loc['907588803161939968', 'polarity'], 0.4)\n",
    "assert np.allclose(polarities.loc['907579024960098304', 'polarity'], 4.7)\n",
    "# If you fail this test, you dropped tweets with 0 polarity\n",
    "assert np.allclose(polarities.loc['907303264458362880', 'polarity'], 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br/> <br/>\n",
    "\n",
    "\n",
    "## Question 5f:\n",
    "\n",
    "Finally, use the `polarities` and `trump` tables to create a new table called `senti` that is the `trump` table with an extra column called `polarity` containing the sentiment polarity of each tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "senti = trump.merge(polarities, left_index=True, right_index=True) # SOLUTION\n",
    "senti.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "tags": [
     "test",
     "q05f"
    ]
   },
   "outputs": [],
   "source": [
    "assert np.allclose(senti.loc['907698529606541312', 'polarity'], 7.3)\n",
    "assert np.allclose(senti.loc['907675638055743489', 'polarity'], 2.9)\n",
    "assert np.allclose(senti.loc['907592460070768641', 'polarity'], 3.2)\n",
    "assert np.allclose(senti.loc['907588803161939968', 'polarity'], 0.4)\n",
    "assert np.allclose(senti.loc['907579024960098304', 'polarity'], 4.7)\n",
    "# If you fail this test, you dropped tweets with 0 polarity\n",
    "assert np.allclose(senti.loc['907303264458362880', 'polarity'], 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a measure of the sentiment of each of his tweets! Note that this calculation is rather basic; you can read over the VADER readme to understand a more robust sentiment analysis.\n",
    "\n",
    "Now, run the cells below to see the most positive and most negative tweets from Trump in your dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Most negative tweets:')\n",
    "for t in senti.sort_values('polarity').head()['text']:\n",
    "    print('  ', t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Most positive tweets:')\n",
    "for t in senti.sort_values('polarity', ascending=False).head()['text']:\n",
    "    print('  ', t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br></br>\n",
    "\n",
    "## Question 5g\n",
    "\n",
    "Use seaborn to create a `distplot` of the sentiments. What do you notice? (You don't have to write this down in the notebook.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution",
     "written"
    ]
   },
   "outputs": [],
   "source": [
    "sns.distplot(senti['polarity'], bins=np.arange(-7, 7, 0.5)) # SOLUTION\n",
    "plt.xlim(-7, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br></br>\n",
    "\n",
    "## Question 5h\n",
    "Create a line plot of the sentiment of Trump's tweets over time. Plot the mean sentiment of each month of his tweets over time. Then, plot vertical lines corresponding to his election and inauguration dates. Your final plot should look like:\n",
    "\n",
    "![](senti_time.png)\n",
    "\n",
    "Then, describe what you notice and save your result in the variable `senti_time_answer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution",
     "written"
    ]
   },
   "outputs": [],
   "source": [
    "(senti\n",
    " .loc[:, ['time', 'polarity']]\n",
    " .set_index('time')\n",
    " .resample('M')\n",
    " .mean()\n",
    " .plot()\n",
    ")\n",
    "plt.axvline(x=ELEC_DATE, color='r', linestyle='--')\n",
    "plt.axvline(x=INAUG_DATE, color='r', linestyle='--')\n",
    "\n",
    "plt.savefig('senti_time.png', bbox_inches='tight')\n",
    "\n",
    "senti_time_answer = '''\n",
    "\n",
    "We notice an increase in positivity before the election, then an almost\n",
    "immediate decline afterwards until soon after his inauguration.\n",
    "\n",
    "'''\n",
    "display(Markdown(senti_time_answer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6: Hashtags\n",
    "\n",
    "Let's return now to the problem of determining which tweets were from Trump and which were from his aides. As a reminder, most analysts agree that tweets that came from an Android device were likely from Trump himself, not one of his aides ([example](http://didtrumptweetit.com/machine-learning-tweet-prediction/)).\n",
    "\n",
    "In addition, browsing his Twitter shows that some tweets that sound more \"official\" often have a hashtag, link, or a picture:\n",
    "\n",
    "![](aide.png)\n",
    "\n",
    "Whereas tweets that sound like Trump himself usually don't have a hashtag, link, or picture:\n",
    "\n",
    "![](trump.png)\n",
    "\n",
    "So, we can hypothesize that if a tweet has a hashtag, link, or picture it came from one of Trump's aides, not Trump himself. Let's see if this idea is backed up by the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br></br>\n",
    "\n",
    "## Question 6a\n",
    "\n",
    "Create a DF called `hash_or_link` that contains only the rows from the `senti` table where the tweet isn't a retweet and contains a hashtag, link, or picture. We say that:\n",
    "\n",
    "- A tweet is a retweet if it has the string 'rt' anywhere in the tweet if it is preceeded and followed by a non-word character (the start and end of the string count as non-word characters).\n",
    "- A tweet has a hashtag if it has the character '#' anywhere in the tweet followed by a letter.\n",
    "- A tweet contains a link or a picture if it has the word `http` anywhere in the tweet\n",
    "\n",
    "(You can check out Trump's Twitter for why these criteria are true)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "rt_re = r'^rt|rt$|\\Wrt\\W'\n",
    "hash_re = r'#[a-z]|http'\n",
    "hash_or_link = senti[~senti['text'].str.contains(rt_re) & senti['text'].str.contains(hash_re)]\n",
    "hash_or_link.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "tags": [
     "test",
     "q06a"
    ]
   },
   "outputs": [],
   "source": [
    "assert isinstance(hash_re, str)\n",
    "assert re.search(rt_re, 'this') is None\n",
    "assert re.search(rt_re, 'this is the start.') is None\n",
    "assert re.search(rt_re, 'rt hi') is not None\n",
    "assert re.search(rt_re, 'hi rt') is not None\n",
    "assert re.search(rt_re, 'rt: hello') is not None\n",
    "assert re.search(rt_re, 'hello rt: @Sam') is not None\n",
    "assert re.search(hash_re, '# heya') is None\n",
    "assert re.search(hash_re, '#') is None\n",
    "assert re.search(hash_re, '#heya') is not None\n",
    "assert re.search(hash_re, '#h') is not None\n",
    "assert re.search(hash_re, 'ds100 is #goals') is not None\n",
    "assert re.search(hash_re, 'ds100 is # goals') is None\n",
    "assert re.search(hash_re, 'http://google.com') is not None\n",
    "assert re.search(hash_re, 'https://google.com') is not None\n",
    "assert re.search(hash_re, 'hihttphello') is not None\n",
    "\n",
    "assert isinstance(hash_or_link, pd.DataFrame)\n",
    "# If you fail this test, you might not be looking everywhere in the string\n",
    "assert len(hash_or_link) > 1100\n",
    "assert '907303264458362880' in hash_or_link.index\n",
    "assert '907311779331690496' in hash_or_link.index\n",
    "assert '907588803161939968' in hash_or_link.index\n",
    "assert '907675638055743489' in hash_or_link.index\n",
    "assert '907698529606541312' in hash_or_link.index\n",
    "assert '906828550065582080' not in hash_or_link.index\n",
    "assert '906828871106002944' not in hash_or_link.index\n",
    "assert '906829008955957248' not in hash_or_link.index\n",
    "assert '907579024960098304' not in hash_or_link.index\n",
    "assert '907592460070768641' not in hash_or_link.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "<br></br>\n",
    "\n",
    "## Question 6b\n",
    "\n",
    "Create a line plot of the number of tweets containing a hashtag, link, or picture from each tweet device.\n",
    "\n",
    "If a device doesn't have at least 20 tweets in a particular year, don't include the tweets from that device for that year. (Eg. if there are 100 tweets from Twitter Ads in 2016 but only 10 in 2017, plot the counts for Twitter Ads in 2016 but not 2017.)\n",
    "\n",
    "Aggregate the counts for each week period. Your final plot should look like:\n",
    "\n",
    "![](hashtag_by_device.png)\n",
    "\n",
    "*Hint:* If you can get a table in the following format, you can just call `.plot()` on it to generate the plot.\n",
    "\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th>source</th>\n",
    "      <th>Media Studio</th>\n",
    "      <th>Twitter Ads</th>\n",
    "      <th>Twitter Web Client</th>\n",
    "      <th>Twitter for Android</th>\n",
    "      <th>Twitter for iPhone</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>time</th>\n",
    "      <th></th>\n",
    "      <th></th>\n",
    "      <th></th>\n",
    "      <th></th>\n",
    "      <th></th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>2016-07-24</th>\n",
    "      <td>NaN</td>\n",
    "      <td>NaN</td>\n",
    "      <td>4.0</td>\n",
    "      <td>10.0</td>\n",
    "      <td>14.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2016-08-07</th>\n",
    "      <td>NaN</td>\n",
    "      <td>NaN</td>\n",
    "      <td>5.0</td>\n",
    "      <td>15.0</td>\n",
    "      <td>91.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2016-08-21</th>\n",
    "      <td>NaN</td>\n",
    "      <td>NaN</td>\n",
    "      <td>9.0</td>\n",
    "      <td>10.0</td>\n",
    "      <td>44.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2016-09-04</th>\n",
    "      <td>NaN</td>\n",
    "      <td>NaN</td>\n",
    "      <td>0.0</td>\n",
    "      <td>20.0</td>\n",
    "      <td>51.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2016-09-18</th>\n",
    "      <td>NaN</td>\n",
    "      <td>NaN</td>\n",
    "      <td>1.0</td>\n",
    "      <td>12.0</td>\n",
    "      <td>59.0</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "What conclusions can you draw from this plot? Does this plot allow us to say whether a tweet containing a hashtag/link/picture likely came from Trump himself? Write your takeaways in `hashtag_answer` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution",
     "written"
    ]
   },
   "outputs": [],
   "source": [
    "(hash_or_link\n",
    " .groupby([hash_or_link['time'].dt.year, 'source'])\n",
    " .filter(lambda group: len(group) >= 20)\n",
    " .set_index('time')\n",
    " .groupby('source')\n",
    " .resample('W')\n",
    " .size()\n",
    " .rename('count')\n",
    " .unstack(level=0)\n",
    " .plot(figsize=(8, 6))\n",
    ")\n",
    "plt.savefig('hashtag_by_device.png', bbox_inches='tight')\n",
    "\n",
    "hashtag_answer = '''\n",
    "\n",
    "Looks like while Trump used Android, he almost never used hashtags or links.\n",
    "Since we know that tweets from an Android were likely from Trump himself, this\n",
    "suggests that tweets that didn't use hashtags or links were likely from him as\n",
    "well.\n",
    "\n",
    "If he still tweets this way, then we can use the presence of hashtags/links/pictures\n",
    "to tell which tweets came from him or not.\n",
    "\n",
    "'''\n",
    "display(Markdown(hashtag_answer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "<br></br>\n",
    "\n",
    "## Question 6c \n",
    "\n",
    "Now, let's see whether there's a difference in sentiment for tweets with hashtags and those without.\n",
    "\n",
    "Create a line plot of the sentiment of Trump's non-retweet tweets over time, taking the mean sentiment for every month. Plot one line for tweets with hashtags and one for tweets without. Then, draw two vertical lines for the election date and inauguration date. Draw a horizontal line for y=0. Your plot should look like:\n",
    "\n",
    "![](hash_senti.png)\n",
    "\n",
    "What do you notice about the plot? What conclusions can you draw? Write your answers in `hash_senti_answer`.\n",
    "\n",
    "Hint 1: You'll want to start use your `senti` table, not your `hash_or_link` table. Try adding a column to `senti` containing True if the tweet has a hashtag/link/picture.\n",
    "\n",
    "Hint 2: You shouldn't use your `no_punc` column. Think about why not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution",
     "written"
    ]
   },
   "outputs": [],
   "source": [
    "non_retweets = senti[~senti['text'].str.contains(rt_re)]\n",
    "\n",
    "(non_retweets\n",
    " .assign(has_hash_or_link=non_retweets['text'].str.contains(hash_re))\n",
    " .loc[:, ['time', 'polarity', 'has_hash_or_link']]\n",
    " .set_index('time')\n",
    " .groupby('has_hash_or_link')\n",
    " .resample('M')\n",
    " .mean()\n",
    " .loc[:, 'polarity']\n",
    " .unstack(level=0)\n",
    " .plot(figsize=(8, 6))\n",
    ")\n",
    "\n",
    "plt.axvline(x=ELEC_DATE, color='r', linestyle='--')\n",
    "plt.axvline(x=INAUG_DATE, color='r', linestyle='--')\n",
    "plt.axhline(color='black')\n",
    "plt.savefig('hash_senti.png', bbox_inches='tight')\n",
    "\n",
    "hash_senti_answer = '''\n",
    "\n",
    "We notice that the sentiments for tweets with a hashtag were almost always higher\n",
    "than tweets without a hashtag. If we believe that Trump didn't usually use hashtags\n",
    "his tweets, this implies that the sentiment of his tweets is more negative than\n",
    "those of his aides.\n",
    "\n",
    "We also notice an increase in both hashtag and non-hashtag tweet sentiment until\n",
    "the election. While the tweets with hashtags generally grew in positivity, the\n",
    "tweets without hashtags dropped greatly in sentiment until the inauguration.\n",
    "\n",
    "'''\n",
    "display(Markdown(hash_senti_answer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## Question 7: Engagement\n",
    "\n",
    "---\n",
    "<br></br>\n",
    "## Question 7a\n",
    "\n",
    "Which of Trump's tweets had the most retweets? Were there certain words that often led to more retweets?\n",
    "\n",
    "We can find this out by using our `tidy_format` DataFrame. For each word in the `tidy_format` DF, find out the number of retweets that its tweet got. Filter out words that didn't appear in at least 25 tweets, find out the median number of retweets each word got, and save the top 20 most retweeted words into a DataFrame called `top_20`. Your `top_20` table should have this format:\n",
    "\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>retweet_count</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>word</th>\n",
    "      <th></th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>fake</th>\n",
    "      <td>22963.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>news</th>\n",
    "      <td>20463.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>ds100</th>\n",
    "      <td>20432.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>great</th>\n",
    "      <td>20159.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>class</th>\n",
    "      <td>20121.0</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "top_20 = (tidy_format\n",
    " .join(trump['retweet_count'])\n",
    " .loc[:, ['word', 'retweet_count']]\n",
    " .groupby('word')\n",
    " .filter(lambda g: len(g) >= 25)\n",
    " .groupby('word')\n",
    " .median()\n",
    " .sort_values('retweet_count', ascending=False)\n",
    " .head(20)\n",
    ")\n",
    "top_20.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "tags": [
     "test",
     "q07a"
    ]
   },
   "outputs": [],
   "source": [
    "# Although it can't be guaranteed, it's very likely that the top 7 words will still be\n",
    "# in the top 20 words in the next month.\n",
    "assert 'fake'     in top_20.index\n",
    "assert 'news'     in top_20.index\n",
    "assert 'korea'    in top_20.index\n",
    "assert 'terrible' in top_20.index\n",
    "assert 'russian'  in top_20.index\n",
    "assert 'story'    in top_20.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a bar chart of your results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_20['retweet_count'].sort_values().plot.barh(figsize=(10, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br></br>\n",
    "\n",
    "## Question 7b\n",
    "\n",
    "The phrase \"fake news\" is apparently really popular! We can conclude that Trump's tweets containing \"fake\" and/or \"news\" result in the most retweets relative to words his other tweets. Or can we?\n",
    "\n",
    "Consider each of the statements about possible confounding factors below. State whether each statement is true or false and explain. If the statement is true, state whether the confounding factor could have made \"fake\" and/or \"news\" higher on our list than they should be.\n",
    "\n",
    "1. We didn't restrict our word list to nouns, so we have unhelpful words like \"let\" and \"any\" in our result.\n",
    "1. We didn't remove hashtags in our text, so we have duplicate words (eg. #great and great).\n",
    "1. We didn't account for the fact that Trump's follower count has increased over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution",
     "written"
    ]
   },
   "outputs": [],
   "source": [
    "engagement_answer = '''\n",
    "\n",
    "1. True. However, this will not cause \"fake news\" to drop on the list of retweeted\n",
    "words since accounting for this means we'll remove words from our list.\n",
    "1. False. We removed hashtags in our text when we removed punctuation.\n",
    "1. True. This could indeed cause \"fake news\" to appear higher on the list than it should have.\n",
    "If his follower count increased over time, we would expect the number of retweets over time\n",
    "to increase as well, regardless of what words are in the tweets. If he just started using the\n",
    "term \"fake news\" recently, it's likely that those tweets would get more retweets just because\n",
    "he had more followers than before.\n",
    "\n",
    "'''\n",
    "display(Markdown(engagement_answer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Question 8: Fake News\n",
    "\n",
    "---\n",
    "<br></br>\n",
    "## Question 8a \n",
    "\n",
    "Let's investigate the term \"fake news\" a bit more. Create a table called `fake_counts` that has two columns:\n",
    "\n",
    "1. `fake_news`: The number of tweets containing the term \"fake news\".\n",
    "1. `total`: The total number of tweets for the time period.\n",
    "\n",
    "The index of the table should be datetimes for each two-week period in the data.\n",
    "\n",
    "The resulting table should look like:\n",
    "\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>fake_news</th>\n",
    "      <th>total</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>time</th>\n",
    "      <th></th>\n",
    "      <th></th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>2016-07-31</th>\n",
    "      <td>0</td>\n",
    "      <td>79</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2016-08-14</th>\n",
    "      <td>0</td>\n",
    "      <td>143</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2016-08-28</th>\n",
    "      <td>0</td>\n",
    "      <td>118</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2016-09-11</th>\n",
    "      <td>0</td>\n",
    "      <td>121</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2016-09-25</th>\n",
    "      <td>0</td>\n",
    "      <td>111</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "fake_counts = (trump[:3100]\n",
    " .set_index('time')\n",
    " .resample('2W')\n",
    " .agg({'text': lambda g: np.count_nonzero(g.str.contains('fake news')), 'retweet_count': len})\n",
    " .rename(columns={'text': 'fake_news', 'retweet_count': 'total'})\n",
    ")\n",
    "fake_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "tags": [
     "test",
     "q08a"
    ]
   },
   "outputs": [],
   "source": [
    "assert isinstance(fake_counts, pd.DataFrame)\n",
    "assert 'fake_news' in fake_counts\n",
    "assert 'total' in fake_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "<br></br>\n",
    "\n",
    "## Question 8b\n",
    "\n",
    "Now, create a line plot showing the proportion of tweets containing the term \"fake news\" over time. Then, draw two vertical lines corresponding to the election and inauguration dates.\n",
    "\n",
    "What do you notice? Write your conclusions in `fake_news_answer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution",
     "written"
    ]
   },
   "outputs": [],
   "source": [
    "(fake_counts['fake_news'] / fake_counts['total']).plot()\n",
    "plt.axvline(x=ELEC_DATE, color='r', linestyle='--')\n",
    "plt.axvline(x=INAUG_DATE, color='r', linestyle='--')\n",
    "\n",
    "fake_news_answer = '''\n",
    "\n",
    "Trump almost never used the term \"fake news\" before election, then he used it a lot!\n",
    "\n",
    "We can conclude that there might have been some news related to the election\n",
    "that Trump disagreed with.\n",
    "\n",
    "'''\n",
    "display(Markdown(fake_news_answer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "\n",
    "Congrats, you just finished Project 1!\n",
    "\n",
    "Run the cell below to run all the OkPy tests at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(\"Running all tests...\")\n",
    "_ = [ok.grade(q[:-3]) for q in os.listdir(\"ok_tests\") if q.startswith('q')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run the cell below to submit your assignment to OkPy. The autograder should email you shortly with your autograded score. The autograder will only run once every 30 minutes.\n",
    "\n",
    "**If you're failing tests on the autograder but pass them locally**, you should simulate the autograder by doing the following:\n",
    "\n",
    "1. In the top menu, click Kernel -> Restart and Run all.\n",
    "2. Run the cell above to run each OkPy test.\n",
    "\n",
    "**You must make sure that you pass all the tests when running steps 1 and 2 in order.** If you are still failing autograder tests, you should double check your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = ok.submit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "## Obama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "obama_re = r'obama|barack'\n",
    "(senti\n",
    " [senti['text'].str.contains(obama_re)]\n",
    " .loc[:, ['time', 'polarity']]\n",
    " .set_index('time')\n",
    " .resample('MS')\n",
    " .mean()\n",
    " .plot()\n",
    ")\n",
    "plt.axvline(x=ELEC_DATE, color='r', linestyle='--')\n",
    "plt.axvline(x=INAUG_DATE, color='r', linestyle='--')\n",
    "plt.axhline(color='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "So Trump tweets containing obama were positive up to the election, then quickly turned more negative. Note that these tweets also contain the word obamacare..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "(senti\n",
    " [senti['text'].str.contains(obama_re)]\n",
    " .sort_values('polarity')\n",
    " .head()\n",
    " ['text']\n",
    " .values\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "(senti\n",
    " [senti['text'].str.contains(obama_re)]\n",
    " .sort_values('polarity', ascending=False)\n",
    " .head()\n",
    " ['text']\n",
    " .values\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "But hmm. Looks like even his positive posts are about getting rid of Obamacare."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}